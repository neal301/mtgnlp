
TF-IDF:
- Term Frequency Inverse Document Frequency
- Compares how many times a word appears in a document to how many times it appears in an entire dataset


BERT embeddings:
- each 'token' (word) is a 768 dimensional vector that contains contextual information
-
Sure, let’s break down the embed_text function:

    tokenizer.encode(text, add_special_tokens=True): This line tokenizes the input text. 
    Tokenization is the process of converting a sequence of text into individual tokens (usually words or subwords). 
    The encode method of the tokenizer not only tokenizes the text but also maps each token to its corresponding ID in the vocabulary of the pretrained BERT model. 
    The add_special_tokens=True argument adds special tokens ([CLS], [SEP], etc.) that are specific to the BERT model. 
    The [CLS] token is added at the beginning of the text and is used as the aggregate sequence representation for classification tasks. The [SEP] token is added at the end of the text.

    torch.tensor(input_ids).unsqueeze(0): This line converts the list of token IDs into a PyTorch tensor so that it can be fed into the BERT model. 
    The unsqueeze(0) function is used to add an extra dimension to the tensor, which represents the batch size. In this case, the batch size is 1 because you’re processing one piece of text at a time.

    with torch.no_grad(): outputs = model(input_ids): This block feeds the input IDs into the pretrained BERT model to get the embeddings. 
    The torch.no_grad() context manager is used to disable gradient calculations during inference, which reduces memory usage and speeds up computation.

    outputs[0][0, 0, :]: The BERT model returns a tuple where the first element, outputs[0], contains the hidden states from the last layer of the model. 
    Each hidden state is a 768-dimensional vector (for the ‘bert-base-uncased’ model) that represents a token in the input text. 
    The [0, 0, :] indexing operation selects the hidden state of the first token of the first sequence, which is the [CLS] token.

    .numpy(): This line converts the PyTorch tensor to a NumPy array. 
    This is done because NumPy arrays are generally easier to work with and more compatible with other Python libraries compared to PyTorch tensors.

    return embeddings: Finally, the function returns the embeddings of the [CLS] token, which can be used as a sentence-level embedding of the input text.

