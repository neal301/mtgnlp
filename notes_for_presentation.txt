
TF-IDF:
- Term Frequency Inverse Document Frequency
- Compares how many times a word appears in a document to how many times it appears in an entire dataset


BERT embeddings:
- each 'token' (word) is a 768 dimensional vector that contains contextual information
-
Sure, let’s break down the embed_text function:

    tokenizer.encode(text, add_special_tokens=True): This line tokenizes the input text. 
    Tokenization is the process of converting a sequence of text into individual tokens (usually words or subwords). 
    The encode method of the tokenizer not only tokenizes the text but also maps each token to its corresponding ID in the vocabulary of the pretrained BERT model. 
    The add_special_tokens=True argument adds special tokens ([CLS], [SEP], etc.) that are specific to the BERT model. 
    The [CLS] token is added at the beginning of the text and is used as the aggregate sequence representation for classification tasks. The [SEP] token is added at the end of the text.

    torch.tensor(input_ids).unsqueeze(0): This line converts the list of token IDs into a PyTorch tensor so that it can be fed into the BERT model. 
    The unsqueeze(0) function is used to add an extra dimension to the tensor, which represents the batch size. In this case, the batch size is 1 because you’re processing one piece of text at a time.

    with torch.no_grad(): outputs = model(input_ids): This block feeds the input IDs into the pretrained BERT model to get the embeddings. 
    The torch.no_grad() context manager is used to disable gradient calculations during inference, which reduces memory usage and speeds up computation.

    outputs[0][0, 0, :]: The BERT model returns a tuple where the first element, outputs[0], contains the hidden states from the last layer of the model. 
    Each hidden state is a 768-dimensional vector (for the ‘bert-base-uncased’ model) that represents a token in the input text. 
    The [0, 0, :] indexing operation selects the hidden state of the first token of the first sequence, which is the [CLS] token.

    .numpy(): This line converts the PyTorch tensor to a NumPy array. 
    This is done because NumPy arrays are generally easier to work with and more compatible with other Python libraries compared to PyTorch tensors.

    return embeddings: Finally, the function returns the embeddings of the [CLS] token, which can be used as a sentence-level embedding of the input text.

- question about shape of vectors:

    You’re correct that each token in BERT does indeed have a 768-dimensional vector associated with it (for BERT-Base). This vector captures the contextual information of that token within the sentence. 
    However, when we’re dealing with tasks that require understanding the entire sentence or paragraph (like text classification or semantic similarity), we need a single vector representation for the whole input.

    The [CLS] token is designed for this purpose. During pre-training, BERT learns to aggregate the context of the entire sentence into the [CLS] token. So, the 768-dimensional vector of the [CLS] 
    token is a summary of the entire sentence, and it’s often used for sentence-level tasks.

    That being said, you’re also correct that some information might be lost when we only use the [CLS] token. If your task requires more fine-grained information (like token-level tasks such as named 
    entity recognition or question answering), you might want to use the individual token embeddings instead of, or in addition to, the [CLS] token.

    In summary, whether to use the [CLS] token or the individual token embeddings (or both) depends on the specific requirements of your task. The [CLS] token provides a compact, sentence-level representation, 
    while the individual token embeddings provide more detailed, token-level information. Both carry contextual information, but at different levels of granularity. 