{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec-google-news-300\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load FastText embeddings from gensim's online repository\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m fasttext_model \u001b[38;5;241m=\u001b[39m api\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasttext-wiki-news-subwords-300\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\Lib\\site-packages\\gensim\\downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    501\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, BASE_DIR)\n\u001b[0;32m    502\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28m__import__\u001b[39m(name)\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39mload_data()\n",
      "File \u001b[1;32m~/gensim-data\\fasttext-wiki-news-subwords-300\\__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[0;32m      7\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasttext-wiki-news-subwords-300\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfasttext-wiki-news-subwords-300.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload_word2vec_format(path, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39mfvocab, binary\u001b[38;5;241m=\u001b[39mbinary, encoding\u001b[38;5;241m=\u001b[39mencoding, unicode_errors\u001b[38;5;241m=\u001b[39municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit, datatype\u001b[38;5;241m=\u001b[39mdatatype, no_header\u001b[38;5;241m=\u001b[39mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m         )\n\u001b[0;32m   2068\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(kv):\n\u001b[0;32m   2071\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2073\u001b[0m         kv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlen\u001b[39m(kv),\n\u001b[0;32m   2074\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1974\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1973\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1974\u001b[0m word, weights \u001b[38;5;241m=\u001b[39m _word2vec_line_to_vector(line, datatype, unicode_errors, encoding)\n\u001b[0;32m   1975\u001b[0m _add_word_to_kv(kv, counts, word, weights, vocab_size)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1980\u001b[0m, in \u001b[0;36m_word2vec_line_to_vector\u001b[1;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_word2vec_line_to_vector\u001b[39m(line, datatype, unicode_errors, encoding):\n\u001b[0;32m   1979\u001b[0m     parts \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(line\u001b[38;5;241m.\u001b[39mrstrip(), encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39municode_errors)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1980\u001b[0m     word, weights \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m0\u001b[39m], [datatype(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m parts[\u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m   1981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word, weights\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1980\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_word2vec_line_to_vector\u001b[39m(line, datatype, unicode_errors, encoding):\n\u001b[0;32m   1979\u001b[0m     parts \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(line\u001b[38;5;241m.\u001b[39mrstrip(), encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39municode_errors)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1980\u001b[0m     word, weights \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m0\u001b[39m], [datatype(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m parts[\u001b[38;5;241m1\u001b[39m:]]\n\u001b[0;32m   1981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m word, weights\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#importations and models\n",
    "\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# Load GloVe embeddings from gensim's online repository\n",
    "glove_word_vectors = api.load('glove-wiki-gigaword-100')\n",
    "\n",
    "# Load Word2Vec embeddings from gensim's online repository\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# Load FastText embeddings from gensim's online repository\n",
    "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original file upload\n",
    "file_path = \"mycards.json\"\n",
    "df = pd.read_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "df = df[df.lang == 'en']\n",
    "df = df[df.layout != 'token']\n",
    "df = df[df.layout != 'art_series']\n",
    "df = df[df.layout != 'double_faced_token']\n",
    "df = df[df.layout != 'vanguard']\n",
    "df = df[df.layout != 'emblem']\n",
    "df = df[df.layout != 'planar']\n",
    "df = df[df.layout != 'scheme']\n",
    "df = df[df.type_line != 'Hero']\n",
    "df = df[df.type_line != 'Conspiracy']\n",
    "df = df[~((df.name == 'Plains') | (df.name == 'Island') | (df.name == 'Swamp') | (df.name == 'Mountain') | (df.name == 'Forest'))]\n",
    "df = df[~((df.name == 'Plains // Plains') | (df.name == 'Island // Island') | (df.name == 'Swamp // Swamp') | (df.name == 'Mountain // Mountain') | (df.name == 'Forest // Forest'))]\n",
    "df=df[df.set_type != 'memorabilia']\n",
    "df = df.drop_duplicates(subset=['name'])\n",
    "df = df[df['set_name'] != 'Unhinged']\n",
    "df = df[df['set_name'] != 'Unglued']\n",
    "df = df[df['set_name'] != 'Unstable']\n",
    "\n",
    "#two-sided\n",
    "df_explode = df[df['name'].str.contains(\"//\")]\n",
    "df_explode = df_explode.explode('card_faces')\n",
    "df_explode.reset_index(drop=True, inplace=True)\n",
    "df_normalized = pd.json_normalize(df_explode['card_faces'])\n",
    "\n",
    "#keeping id numbers\n",
    "df_normalized['id']=df_explode['id']\n",
    "df_normalized['color_identity']=df_explode['color_identity']\n",
    "df_normalized['colors']=df_explode['colors']\n",
    "df_normalized['keywords']=df_explode['keywords']\n",
    "df_normalized['legalities']=df_explode['legalities']\n",
    "\n",
    "# Concatenate the normalized DataFrame with the original DataFrame\n",
    "df = pd.concat([df, df_normalized], ignore_index=True)\n",
    "\n",
    "#dropping duals after 2-sided split\n",
    "df=df[~df.name.str.contains('//')]\n",
    "\n",
    "#column dropping\n",
    "cols_to_drop = np.array(df.columns[[0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 15, 16, 23, 27, 28, 29, 30, 31, 32, 33] + list(range(34, 85))])\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "cols_to_drop = np.array(df.columns[list(range(12, 20))])\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "#further cleaning\n",
    "df['oracle_text'] = df['oracle_text'].fillna('')\n",
    "df['power'] = df['power'].fillna(0)\n",
    "df['toughness'] = df['toughness'].fillna(0)\n",
    "df['power'] = pd.to_numeric(df['power'], errors='coerce')\n",
    "df['toughness'] = pd.to_numeric(df['toughness'], errors='coerce')\n",
    "power_mean = df['power'].mean()\n",
    "toughness_mean = df['toughness'].mean()\n",
    "df['power'].fillna(power_mean, inplace=True)\n",
    "df['toughness'].fillna(toughness_mean, inplace=True)\n",
    "\n",
    "#silly card\n",
    "df=df[~df.name.str.contains('The Ultimate Nightmare of Wizards of the Coast® Customer Service')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n",
      "Unexpected color: 2\n"
     ]
    }
   ],
   "source": [
    "#getting mana counts\n",
    "def count_mana_symbols(text):\n",
    "    counts = {'colorless': 0, 'U': 0, 'G': 0, 'W': 0, 'B': 0, 'R': 0, 'X': 0, 'C' :0, 'P':0, 'S':0}\n",
    "    pattern = r'{([^}]*)}'\n",
    "    mana_symbols = re.findall(pattern, text)\n",
    "    for symbol in mana_symbols:\n",
    "        if symbol.isdigit():\n",
    "            counts['colorless'] += int(symbol)\n",
    "        elif '/' in symbol:\n",
    "            colors = symbol.split('/')\n",
    "            for color in colors:\n",
    "                if color in counts:\n",
    "                    counts[color] += 0.5\n",
    "                else:\n",
    "                    print(f\"Unexpected color: {color}\")\n",
    "        elif symbol in counts:\n",
    "            counts[symbol] += 1\n",
    "        else:\n",
    "            print(f\"Unexpected symbol: {symbol}\")\n",
    "    \n",
    "    return counts\n",
    "df = df.join(df['mana_cost'].astype(str).apply(lambda x: pd.Series(count_mana_symbols(x))))\n",
    "\n",
    "#filling cmc\n",
    "df.loc[df.cmc.isna(), 'cmc'] = df[['colorless', 'U', 'G', 'W', 'B', 'R', 'X', 'C', 'P', 'S']].sum(axis=1)\n",
    "\n",
    "column_mapping = {\n",
    "    'colorless': 'colorless_int',\n",
    "    'U': 'U_int',\n",
    "    'G': 'G_int',\n",
    "    'W': 'W_int',\n",
    "    'B': 'B_int',\n",
    "    'R': 'R_int',\n",
    "    'X': 'X_int',\n",
    "    'C': 'C_int',\n",
    "    'P': 'P_int',\n",
    "    'S': 'S_int'\n",
    "}\n",
    "\n",
    "# Rename the columns using the mapping\n",
    "df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Define a list of all possible colors\n",
    "all_colors = ['W', 'U', 'B', 'R', 'G']\n",
    "\n",
    "# Create separate binary columns for each color and one for \"None\"\n",
    "for color in all_colors + ['None']:\n",
    "    df[f'color_{color}'] = df['color_identity'].apply(lambda x: 1 if color in x else 0 if color in x else 1 if x == [] else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "keyword_counts = Counter(keyword for sublist in df['keywords'] for keyword in sublist)\n",
    "selected_keywords = [keyword for keyword, count in keyword_counts.items() if count > 50]\n",
    "for keyword in selected_keywords:\n",
    "    df[keyword] = df['keywords'].apply(lambda x: 1 if keyword in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, float):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = re.sub(r'{[^}]+}', 'symbol', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text based on word boundaries\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    # Specify multi-word phrases to be merged\n",
    "    multi_word_phrases = ['first strike', 'double strike']  # Add other phrases as needed\n",
    "    \n",
    "    # Merge consecutive tokens forming multi-word phrases\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and ' '.join([tokens[i], tokens[i+1]]) in multi_word_phrases:\n",
    "            merged_tokens.append(' '.join([tokens[i], tokens[i+1]]))\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    return merged_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain embeddings using GloVe\n",
    "def get_glove_embedding(tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embeddings.append(glove_word_vectors[token])\n",
    "        except KeyError:\n",
    "            # Handle out-of-vocabulary tokens\n",
    "            pass\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(glove_word_vectors.vector_size)\n",
    "\n",
    "# Function to obtain embeddings using Word2Vec\n",
    "def get_word2vec_embedding(tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in word2vec_model:\n",
    "            embeddings.append(word2vec_model[token])\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "# Function to obtain embeddings using FastText\n",
    "def get_fasttext_embedding(tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in fasttext_model:\n",
    "            embeddings.append(fasttext_model[token])\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(fasttext_model.vector_size)\n",
    "\n",
    "# Apply preprocessing and get embeddings for each embedding type\n",
    "df['oracle_text_tokens'] = df['oracle_text'].apply(preprocess_text)\n",
    "df['type_line_tokens'] = df['type_line'].apply(preprocess_text)\n",
    "df['glove_embedding'] = df['oracle_text_tokens'].apply(get_glove_embedding)\n",
    "df['glove_embedding_type_line'] = df['type_line_tokens'].apply(get_glove_embedding)\n",
    "df['word2vec_embedding'] = df['oracle_text_tokens'].apply(get_word2vec_embedding)\n",
    "df['word2vec_embedding_type_line'] = df['type_line_tokens'].apply(get_word2vec_embedding)\n",
    "df['fasttext_embedding'] = df['oracle_text_tokens'].apply(get_fasttext_embedding)\n",
    "df['fasttext_embedding_type_line'] = df['type_line_tokens'].apply(get_fasttext_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"cards_clean_final.csv\", index=False, encoding='utf-8')\n",
    "df.to_pickle(\"cards_clean_final.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_no_work=pd.read_csv(\"cards_clean_final.csv\", encoding='utf-8')\n",
    "\n",
    "# Read DataFrame back from the pickle file\n",
    "df = pd.read_pickle(\"cards_clean_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors()"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 1. Separate features into different groups\n",
    "numerical_features = df[['cmc', 'power', 'toughness', 'colorless_int', 'U_int', 'G_int', 'W_int', 'B_int', 'R_int', 'X_int', 'C_int', 'P_int', 'S_int']]\n",
    "binary_features = df[list(df.columns[22:86])]\n",
    "text_embeddings = df[list(df.columns[88:94])]\n",
    "\n",
    "# 2. Normalize numerical features\n",
    "scaler = StandardScaler()\n",
    "normalized_numerical_features = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# 3. Reshape embedding features\n",
    "reshaped_glove_embedding = np.array(df['glove_embedding'].tolist())\n",
    "reshaped_word2vec_embedding = np.array(df['word2vec_embedding'].tolist())\n",
    "reshaped_fasttext_embedding = np.array(df['fasttext_embedding'].tolist())\n",
    "\n",
    "# 4. Concatenate features\n",
    "X = np.concatenate([\n",
    "    normalized_numerical_features, \n",
    "    binary_features.values, \n",
    "    reshaped_glove_embedding,\n",
    "    reshaped_word2vec_embedding,\n",
    "    reshaped_fasttext_embedding\n",
    "], axis=1)\n",
    "\n",
    "# 5. Train KNN model\n",
    "knn = NearestNeighbors(n_neighbors=5, algorithm='auto')\n",
    "knn.fit(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                Fury Sliver\n",
       "19971       Battering Sliver\n",
       "17850          Battle Sliver\n",
       "27176         Cyclops Tyrant\n",
       "9530     Bonesplitter Sliver\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_features_for_card(card_name):\n",
    "    # Extract features for the given card name\n",
    "    card_numerical_features = df.loc[df['name'] == card_name, numerical_features.columns].values\n",
    "    card_binary_features = df.loc[df['name'] == card_name, binary_features.columns].values\n",
    "    card_text_embeddings = df.loc[df['name'] == card_name, text_embeddings.columns].values\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    card_numerical_features_normalized = scaler.transform(card_numerical_features)\n",
    "    \n",
    "    # Reshape embedding features\n",
    "    reshaped_glove_embedding = np.array(df.loc[df['name'] == card_name, 'glove_embedding'].tolist())\n",
    "    reshaped_word2vec_embedding = np.array(df.loc[df['name'] == card_name, 'word2vec_embedding'].tolist())\n",
    "    reshaped_fasttext_embedding = np.array(df.loc[df['name'] == card_name, 'fasttext_embedding'].tolist())\n",
    "\n",
    "    # Concatenate features\n",
    "    card_features = np.concatenate([\n",
    "        card_numerical_features_normalized, \n",
    "        card_binary_features, \n",
    "        reshaped_glove_embedding,\n",
    "        reshaped_word2vec_embedding,\n",
    "        reshaped_fasttext_embedding\n",
    "    ], axis=1)\n",
    "    \n",
    "    return card_features\n",
    "\n",
    "def find_similar_cards(card_name, k=5):\n",
    "    # Get features for the input card\n",
    "    input_card_features = get_features_for_card(card_name)\n",
    "    \n",
    "    # Find most similar cards to the input card\n",
    "    distances, indices = knn.kneighbors(input_card_features)\n",
    "    \n",
    "    # Output most similar cards\n",
    "    similar_cards = df.iloc[indices[0]]\n",
    "    \n",
    "    return similar_cards\n",
    "\n",
    "# Example usage:\n",
    "input_card_name = \"Fury Sliver\"\n",
    "similar_cards = find_similar_cards(input_card_name)\n",
    "similar_cards['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "if 'oracle_text_preprocessed' not in df.columns:\n",
    "    df['oracle_text_preprocessed'] = df['oracle_text'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "def encode_text_with_bert(text):\n",
    "    if isinstance(text, str) and text.strip():  # Check if the input text is a non-empty string\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if tokens:  # Check if tokens are generated\n",
    "            inputs = tokenizer.encode_plus(tokens, return_tensors='pt', padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                embeddings = outputs[0].mean(dim=1)  # Access the output tensor and use the mean of the hidden states as embeddings\n",
    "            return embeddings.numpy()\n",
    "    # Return zeros if the input text is not valid for encoding\n",
    "    return np.zeros((1, model.config.hidden_size))\n",
    "\n",
    "df['bert_embedding'] = df['oracle_text_preprocessed'].apply(encode_text_with_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
